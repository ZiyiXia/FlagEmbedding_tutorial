{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d34077d7-dd3a-49d7-b87b-2914d52992a6",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0755e6-6ef9-4eb9-b3e6-131a0820ffc7",
   "metadata": {},
   "source": [
    "For text retrieval, pattern matching is the most intuitive way. People would use certain characters, words, phrases, or sentence patterns. however, not only for human, it is also extremely inefficient for computer to do pattern matching between a query and a collection of text files to find the possible result. For images and acoustic waves, there are rgb pixels and digital signals. Similarly, we need a way to represent text data. That's how text embedding comes in front of the stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45d0491",
   "metadata": {},
   "source": [
    "## 1. Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18be671d-0c5f-4f9a-914b-079cca961280",
   "metadata": {},
   "source": [
    "Based on the statistical features of words in sentenses/documents, techniques like *one-hot encoding* and *bag-of-words (BoW)* represent words and sentences as sparse vector. While they already provides some These classical approaches only consider the appearance and frequency of words in a single document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc74f1f-4599-4bdb-ac17-b85e286d2b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of one-hot encoding\n",
    "words = ['code', 'phone', 'cup', 'apple']\n",
    "code = [1, 0, 0, 0]\n",
    "phone = [0, 1, 0, 0]\n",
    "cup = [0, 0, 1, 0]\n",
    "apple = [0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6871d84-e741-48c3-8ee4-9c540cbcb04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of bag-of-words\n",
    "sentence1 = \"I love basketball\"\n",
    "sentence2 = \"I have a basketball match\"\n",
    "\n",
    "words = ['I', 'love', 'basketball', 'have', 'a', 'match']\n",
    "sen1_vec = [1, 1, 1, 0, 0, 0]\n",
    "sen2_vec = [1, 0, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9741a5-d86b-4ba9-b671-d103f3a6282e",
   "metadata": {},
   "source": [
    "\n",
    "*TF-IDF* considers how important a word in a document with respect to the whole corpus, which helps out the applications of text classification and filtering. *BM25* is one of the well known ranking algorithm based on TF-IDF.\n",
    "*N-gram* method captures the order of words in a window with size $n$, making a step forward from individual words to a group of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9068ee-35a5-4b45-ba45-39bc2faa6e0e",
   "metadata": {},
   "source": [
    "$$\\text{tf-idf}(t,d,D)=\\text{tf}(t,d)\\cdot\\text{idf}(t,D)$$\n",
    "Where:\n",
    "$$\\text{tf}(t,d)=\\frac{f_{t,d}}{\\sum_{t'\\in{d}} f_{t',d}}$$\n",
    "$$\\text{idf}(t,D)=\\log{\\frac{N}{|\\{d:d\\in{D}\\text{ and }t\\in{d}\\}|}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0494b595-b0b0-447d-89fa-013ac21476c2",
   "metadata": {},
   "source": [
    "However, there's still a long journey ahead for bridging computer to human's natural language. The shortcomings cannot be ignored. First, these methods are facing the \"curse of dimensionality\". It's hard to scale up with the growing size of datasets and limitation of computing power.\n",
    "Besides that, what about the words like \"cat\", \"kitty\", and \"feline\" that are sharing similar semantic sense but with totally different lexical formulation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d34c988-e445-444d-9ee3-e37351e70ec2",
   "metadata": {},
   "source": [
    "To work beyond the limitations, researchers came up with dense word embedding. The key idea is mapping each word to a vector in a low-dimensional space, which could somehow capture the semantic and relational information of the words. The rising of neural network provides a perfect way to build up the model. People can tune the network structure and number of parameters to fit their affordable computing power and training time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef193721-1cc5-401d-81c8-52e8b41f5af3",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OEmWDt4eztOcm5pr2QbxfA.png\" width = 750>\n",
    "    <figcaption>Fig.1 - Word2Vec (<a href=\"https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8\">source</a>)</figcaption>\n",
    "<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a111ebbf-9a33-47c0-90eb-ed1fa05fe89c",
   "metadata": {
    "tags": []
   },
   "source": [
    "The work of *Word2Vec* shows the magic of dense embedding. It use a relatively small neural network with only one hidden layer to find the embedding of words containing the hidden semantic information, with the famous example: \n",
    "$$king - man + woman \\approx queen$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f2fba7-cded-48b1-ae1a-a2e333c58535",
   "metadata": {},
   "source": [
    "Follow by that, with the development of new neural network structures (RNN, CNN, LSTM, etc) that performs well in NLP tasks, more works on dense embedding show good progress. Models like *CoVe*, *ULMFiT*, and *ELMo* based on variance of LSTM all show great results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a71ddb9-4607-4f89-9d2d-f28f029c21e1",
   "metadata": {},
   "source": [
    "**Transformer**, the key architecture of most of the current LLMs, better improves LSTM's advantage of memorizing the context and words relationships. It further supports parallel training on GPU with tensor operations. In this case, we are able to train **larger** models with **larger** datasets. Pre-trained language models like *BERT*, *RoBERTa*, *T5*, and *GPT* were used for encoding tasks.\n",
    "Based on that, models and paradigms like *Sentence-BERT*, *Condenser*, and *RetroMAE* builds bi-encoder and cross-encoder with SOTA performance on on semantic textual similarity benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da26b169",
   "metadata": {},
   "source": [
    "Now into the era of large language models, we've seen the potential of scaling up the model size and training data size to accomplish more sophisticated tasks. So do embedding models. Based on different base models with reasonable model sizes, we can train embedding models to achieve multi functionality, multilingual, multi granularity, in-context learning ability, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d205a711-302e-4e95-a1b8-e38717621e13",
   "metadata": {},
   "source": [
    "## 2. BGE Embedding Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ec99cb-12c3-495a-a9c9-44e21d36efc6",
   "metadata": {},
   "source": [
    "BGE stands for **B**AAI **G**eneral **E**mbedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3dcb05",
   "metadata": {},
   "source": [
    "Embedding Models:\n",
    "\n",
    "| Model  | Language |   Parameters   |   Model Size   |    Description    |   Base Model     |\n",
    "|:-------|:--------:|:--------------:|:--------------:|:-----------------:|:----------------:|\n",
    "| [BAAI/bge-en-icl](https://huggingface.co/BAAI/bge-en-icl) | English | 7.11B |  28.5 GB  | A LLM-based embedding model with in-context learning capabilities, which can fully leverage the model's potential based on a few shot examples | Mistral-7B |\n",
    "| [BAAI/bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2) |  Multilingual   |  9.24B  |  37 GB  | A LLM-based multilingual embedding model, trained on a diverse range of languages and tasks. |    Gemma2-9B    |\n",
    "| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     |   568M   |  2.27 GB  |  Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) | XLM-RoBERTa |\n",
    "| [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)             |   English | 109M |  438 MB  |      a unified embedding model to support diverse retrieval augmentation needs for LLMs       | BERT |\n",
    "\n",
    "\n",
    "\n",
    "BGE v1.5\n",
    "| Model  | Language |   Parameters   |   Model Size   |    Description    |   Base Model     |\n",
    "|:-------|:--------:|:--------------:|:--------------:|:-----------------:|:----------------:|\n",
    "| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5)   | English |    335M    |    1.34 GB   |     version 1.5 with more reasonable similarity distribution      |   BERT   |\n",
    "| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)     | English |    109M    |    438 MB    |     version 1.5 with more reasonable similarity distribution      |   BERT   |\n",
    "| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)   | English |    33.4M   |    133 MB    |     version 1.5 with more reasonable similarity distribution      |   BERT   |\n",
    "| [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5)   | Chinese |    326M    |    1.3 GB    |     version 1.5 with more reasonable similarity distribution      |   BERT   |\n",
    "| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5)     | Chinese |    102M    |    409 MB    |     version 1.5 with more reasonable similarity distribution      |   BERT   |\n",
    "| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5)   | Chinese |    24M     |    95.8 MB   |     version 1.5 with more reasonable similarity distribution      |   BERT   |\n",
    "\n",
    "BGE v1.0\n",
    "| Model  | Language |   Parameters   |   Model Size   |    Description    |   Base Model     |\n",
    "|:-------|:--------:|:--------------:|:--------------:|:-----------------:|:----------------:|\n",
    "| [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en)   | English |    500M    |    1.34 GB   |              Embedding Model which map text into vector                            |  BERT  |\n",
    "| [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en)     | English |    109M    |    438 MB    |          a base-scale model but with similar ability to `bge-large-en`  |  BERT  |\n",
    "| [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en)   | English |    33.4M   |    133 MB    |          a small-scale model but with competitive performance                    |  BERT  |\n",
    "| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh)   | Chinese |    326M    |    1.3 GB    |              Embedding Model which map text into vector                            |  BERT  |\n",
    "| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh)     | Chinese |    102M    |    409 MB    |           a base-scale model but with similar ability to `bge-large-zh`           |  BERT  |\n",
    "| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh)   | Chinese |    24M     |    95.8 MB   |           a small-scale model but with competitive performance                    |  BERT  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9847a9b1",
   "metadata": {},
   "source": [
    "Now, install the FlagEmbedding package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e93d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U FlagEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c57437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are unable to connect to Hugging Face, uncomment the following line to use the mirror\n",
    "\n",
    "# import os\n",
    "# os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4e00cc",
   "metadata": {},
   "source": [
    "The following blocks give very simple examples to show how BGE embedding model works. Note that it might take a while to download the model if you are the first time using it. \n",
    "\n",
    "bge-base-en-v1.5 is about 438 MB. Feel free to play with other models mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "813c9a8a-c15b-4fce-99f1-3a659bb4e2ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T07:43:25.067906Z",
     "iopub.status.busy": "2024-08-08T07:43:25.067185Z",
     "iopub.status.idle": "2024-08-08T07:43:53.733205Z",
     "shell.execute_reply": "2024-08-08T07:43:53.732290Z",
     "shell.execute_reply.started": "2024-08-08T07:43:25.067872Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "\n",
    "model = FlagModel('BAAI/bge-base-en-v1.5',\n",
    "                  query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n",
    "                  use_fp16=True)\n",
    "# Setting use_fp16 to True speeds up computation with a slight performance degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5873d559",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_1 = [\"A cat is playing with yarn\", \"Today's lunch lunch is pizza\"]\n",
    "sentences_2 = [\"She has three kittens\", \"He orders Dominos takeout everyday\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "821bda55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the embedding:  (2, 768)\n"
     ]
    }
   ],
   "source": [
    "embeddings_1 = model.encode(sentences_1)\n",
    "embeddings_2 = model.encode(sentences_2)\n",
    "print(\"shape of the embedding: \", embeddings_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a0fe0b",
   "metadata": {},
   "source": [
    "Here we directly use dot product to compute similarity. More "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "deb2dfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity: \n",
      " [[0.58180463 0.34935987]\n",
      " [0.4080751  0.62310696]]\n"
     ]
    }
   ],
   "source": [
    "similarity = embeddings_1 @ embeddings_2.T\n",
    "print(\"similarity: \\n\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f139da78",
   "metadata": {},
   "source": [
    "For s2p(short query to long passage) retrieval task, suggest to *use encode_queries()* which will automatically add the instruction to each query.\n",
    "\n",
    "Corpus in retrieval task can still use *encode()* or *encode_corpus()*, since they don't need instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "605badfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of queries embedding:    (2, 768)\n",
      "shape of passages emebedding:  (2, 768)\n"
     ]
    }
   ],
   "source": [
    "queries = ['What is panda?', 'What is tiger?']\n",
    "passages = [\n",
    "    \"The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\", \n",
    "    \"The tiger (Panthera tigris) is a member of the genus Panthera and the largest living cat species native to Asia.\"\n",
    "    ]\n",
    "q_embeddings = model.encode_queries(queries)\n",
    "p_embeddings = model.encode(passages)\n",
    "print(\"shape of queries embedding:   \", q_embeddings.shape)\n",
    "print(\"shape of passages emebedding: \", p_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a555f8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.73488915 0.45358998]\n",
      " [0.4899818  0.69096196]]\n"
     ]
    }
   ],
   "source": [
    "scores = q_embeddings @ p_embeddings.T\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9a4454",
   "metadata": {},
   "source": [
    "## 3. Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2a5cc3",
   "metadata": {},
   "source": [
    "We've shown in the code blocks above that getting the similarity of two embedding vector by simply calculating their dot product. There are more applications that embedding are widely used in:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801e91ee",
   "metadata": {},
   "source": [
    "### 3.1 Search Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876dfb72",
   "metadata": {},
   "source": [
    "This is a very important task that information retrieval plays an important role. Embeddings can help match user queries with relevant documents/images/videos by comparing the similarity between the query and the documents/images/videos in the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bfbc17",
   "metadata": {},
   "source": [
    "### 3.2 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534b3183",
   "metadata": {},
   "source": [
    "- **Topic classification**. Embedding can help identify the topic or category of a given piece of text, such as news articles, academic papers, or social media posts.\n",
    "- **Sentiment analysis**. Embeddings are used to classify sentences or texts as positive, negative, or neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d62e78c",
   "metadata": {},
   "source": [
    "### 3.3 Translation and Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7f27a9",
   "metadata": {},
   "source": [
    "Sentence embeddings can assist in aligning sentences across languages by capturing their underlying semantic meanings and contexts. With that, it can be used to translate to other languages, or do summarization/paraphrase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9690deaf",
   "metadata": {},
   "source": [
    "### 3.4 Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b872c7c4",
   "metadata": {},
   "source": [
    "By the characteristic of encoding sentenses to high dimension space, embeddings can be used to group similar sentences or documents together based on their semantic similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03419e18",
   "metadata": {},
   "source": [
    "### 3.5 Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30d1e17",
   "metadata": {},
   "source": [
    "Embeddings can be used to recommend similar content based on the semantic similarity of the user’s past interactions or preferences. It could also help tailor search results based on the user’s history and preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93084df-3392-4288-8151-24bad866a553",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172407d9-ff39-4e2e-9978-ec3090ab0e7a",
   "metadata": {},
   "source": [
    "- [Retrieve Anything To Augment Large Language Models](https://arxiv.org/abs/2310.07554)\n",
    "- [M3-Embedding: Multi-Linguality, Multi-Functionality, Multi-Granularity\n",
    "Text Embeddings Through Self-Knowledge Distillation](https://arxiv.org/pdf/2402.03216)\n",
    "- [A Survey of Text Representation and Embedding Techniques in NLP](https://ieeexplore.ieee.org/abstract/document/10098736)\n",
    "- [Dense Text Retrieval Based on Pretrained Language Models: A Survey](https://dl.acm.org/doi/full/10.1145/3637870?casa_token=3L7XtUgnci8AAAAA%3A2FcXrFQukPQrJEz6czKR-GAfEH4_aE9yoQWdGicIkFUQ2_SYbKDx_iQCn9_afoLgabJNk41BLpLz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1ea5a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f671c1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros((8,5))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d2a884f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.reshape(-1, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
